##################################################
######  Configuration Options for HYPERION #######
##################################################

#============================
# SIMULATION & INFERENCE 
#============================
detectors: ["L1", "H1", "V1"]

reference_gps_time: 1370692818 #1242442967.4 #1370692818

fs: 2048 #sampling frequency Hz for simulations & inference 

ASD_reference_run: O3_GW190521 #reference run for the ASDs

use_reference_asd: False #use the reference run for the ASDs

#inference_parameters: ["M", "q", "e0", "p_0", "distance", "polarization", "inclination", "time_shift", "ra", "dec"]
#inference_parameters: ["M", "q", "e0", "p_0", "distance", "time_shift", "ra", "dec"]

#waveform: "EffectiveFlyByTemplate"
waveform_model: "TEOBResumSDALI"

prior: "TEOBResumS_hyperbolic"

inference_parameters: ["M", "q", "H_hyp", "r_hyp", "j_hyp", "distance", "inclination", "polarization", "coalescence_angle","tcoal", "ra", "dec"]
#inference_parameters: ["M", "q", "ecc", "distance", "inclination", "polarization", "coalescence_angle", "tcoal", "ra", "dec"]

duration: 4 #duration in seconds for the simulated signals


#============================
# FLOW and EMBEDDING NETWORK
#============================
flow:
  num_coupling_layers: 32

coupling_layers:
  num_features:     12
  num_identity:     6
  num_transformed:  6

base_distribution:
  dim:              12
  trainable:        false

embedding_network:
  model:            "CNN+ResNet+Attention"
  kwargs:
    num_blocks:       5
    block_dims:       [2048, 1024, 512, 256]
    strain_out_dim:   256
    use_batch_norm:   true
    dropout_probability: 0.2

    #slicer kwargs
    overlap : 15
    segment_len: 0.2

    #attention kwargs
    num_heads: 32

    CNN_filters:      [32, 64, 128]
    CNN_kernel_sizes: [5, 5, 5]
    #CNN_localization_filters:      [16, 32, 16, 32, 64, 128]
    #CNN_localization_kernel_sizes: [128, 64, 32, 16, 8, 4]
    #CNN_localization_kernel_sizes: [7, 7, 5, 5, 3, 3]


#============================
# TRAINING options
#============================
training_options:
  num_epochs: 300

  batch_size: 256

  initial_learning_rate: 0.0001

  steps_per_epoch: 1000
  val_steps_per_epoch: 150

  num_preload_train: 1000
  num_preload_val: 500

  n_proc: os.cpu_count()

  add_noise: True

  lr_schedule:
    #scheduler: CosineAnnealingLR
    #kwargs:
    #  T_max: 300 #has to be equal to num_epochs
    
    scheduler: ReduceLROnPlateau
    kwargs:
      factor:    0.5
      patience:  20
      mode:      min
      threshold: 0
    
  optimizer:
    algorithm: Adam

  seeds:
    train:   123
    val:     1234
    test:    12345
  verbose:   true

